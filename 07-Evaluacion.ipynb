{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recuerde no agregar o quitar celdas en este notebook, ni modificar su tipo. Si lo hace, el sistema automaticamente lo calificará con cero punto cero (0.0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver los problemas presentados, use el siguiente conjunto de datos. Use [gradetool](gradetool.md) para verificar las respuestas del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data.tsv\n",
    "E\tb,g,f\tjjj#3,bbb#0,ddd#9,ggg#8,hhh#2\n",
    "A\ta,f,c\tccc#2,ddd#0,aaa#3,hhh#9\n",
    "B\tf,e,a,c\tddd#2,ggg#5,ccc#6,jjj#1\n",
    "A\ta,b\thhh#9,iii#5,eee#7,bbb#1\n",
    "C\tf,g,d,a\tiii#6,ddd#5,eee#4,jjj#3\n",
    "A\tc,d\tbbb#2,hhh#0,ccc#4,fff#1,aaa#7\n",
    "A\tg,d,a\taaa#5,fff#8,ddd#2,iii#0,jjj#7,ccc#1\n",
    "B\tb,a\tfff#3,hhh#1,ddd#2\n",
    "E\td,e,a,f\teee#4,ccc#5,iii#9,fff#7,ggg#6,bbb#0\n",
    "B\td,b,g,f\tbbb#7,jjj#9,fff#5,iii#4,ggg#2,eee#3\n",
    "C\td,c,f,b\thhh#6,eee#4,iii#0,fff#2,jjj#1\n",
    "C\td,e,a,c\tbbb#7,iii#6,ggg#9\n",
    "D\tg,e,f,b\tbbb#9,aaa#3,ccc#6,fff#4,eee#2\n",
    "E\tc,f\taaa#8,ddd#5,jjj#1\n",
    "B\td,b\tccc#0,jjj#6,fff#7,ddd#3,aaa#2\n",
    "D\tf,e\tccc#0,eee#6,bbb#9,ddd#3\n",
    "E\te,b,f\tbbb#6,iii#3,hhh#5,fff#4,ggg#9,ddd#2\n",
    "D\tg,a\thhh#4,jjj#5,ccc#9\n",
    "E\te,c,f,a\tccc#1,iii#6,fff#9\n",
    "E\te,a\tbbb#9,aaa#3,fff#1\n",
    "E\te,f\tddd#9,iii#2,aaa#4\n",
    "E\tc,b,g\tccc#5,fff#8,iii#7\n",
    "D\tc,f,a\teee#3,jjj#2,ddd#7\n",
    "A\tf,a,d\tjjj#1,ggg#0,ccc#7,ddd#9,bbb#3\n",
    "E\tc,d\tjjj#6,ccc#0,aaa#1,hhh#9,iii#7,ggg#8\n",
    "E\te,d,c\tfff#3,eee#6,iii#4,bbb#7,ddd#0,ccc#1\n",
    "A\ta,e,f\tfff#0,ddd#5,ccc#4\n",
    "E\tc,a,g\tggg#6,hhh#3,ddd#9,ccc#0,jjj#7\n",
    "A\tf,e\thhh#6,jjj#0,eee#5,iii#7,ccc#3\n",
    "C\tf,c,a,g\teee#1,fff#4,aaa#2,ccc#7,ggg#0,ddd#6\n",
    "A\tb,f\tccc#6,aaa#9,eee#5,ddd#0,bbb#3\n",
    "D\tb,f\tbbb#7,hhh#1,aaa#6,iii#4,fff#9,ddd#5\n",
    "E\ta,c\tfff#3,ccc#1,ggg#2,eee#5\n",
    "B\tb,f,c\tiii#7,ggg#3,ddd#0,jjj#8,hhh#5,ccc#1\n",
    "B\tf,a,e\thhh#6,ccc#3,jjj#0,bbb#8,ddd#7\n",
    "D\ta,f\taaa#0,fff#5,ddd#3\n",
    "B\tc,a\tddd#5,jjj#2,iii#7,ccc#0,bbb#4\n",
    "C\tc,a,e,f\teee#0,fff#2,hhh#6\n",
    "E\te,d\tfff#9,iii#2,eee#0\n",
    "E\tf,a,d\thhh#8,ggg#3,jjj#5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\r\n",
      "SLF4J: Found binding in [jar:file:/home/marvin/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\n",
      "SLF4J: Found binding in [jar:file:/home/marvin/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\r\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\r\n",
      "Hive Session ID = 3e5e3ba5-6469-4fc4-a690-33f2d510652f\r\n",
      "\r\n",
      "Logging initialized using configuration in jar:file:/home/marvin/hive/lib/hive-common-3.1.0.jar!/hive-log4j2.properties Async: true\r\n",
      "Hive Session ID = 3a00888c-bc33-4d9c-9a1c-8812008e55a9\r\n",
      "Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext bigdata\n",
    "%hive_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS t0;\n",
      "OK\n",
      "Time taken: 1.845 seconds\n",
      "CREATE TABLE t0 (\n",
      "    c1 STRING,\n",
      "    c2 ARRAY<CHAR(1)>, \n",
      "    c3 MAP<STRING, INT>\n",
      ")\n",
      "ROW FORMAT DELIMITED \n",
      "FIELDS TERMINATED BY '\\t'\n",
      "COLLECTION ITEMS TERMINATED BY ','\n",
      "MAP KEYS TERMINATED BY '#'\n",
      "LINES TERMINATED BY '\\n';\n",
      "OK\n",
      "Time taken: 0.453 seconds\n",
      "\n",
      "LOAD DATA LOCAL INPATH 'data.tsv' INTO TABLE t0;\n",
      "Loading data to table default.t0\n",
      "OK\n",
      "Time taken: 0.286 seconds\n",
      "\n",
      "-- SELECT * FROM t0;\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS t0;\n",
    "CREATE TABLE t0 (\n",
    "    c1 STRING,\n",
    "    c2 ARRAY<CHAR(1)>, \n",
    "    c3 MAP<STRING, INT>\n",
    ")\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY '\\t'\n",
    "COLLECTION ITEMS TERMINATED BY ','\n",
    "MAP KEYS TERMINATED BY '#'\n",
    "LINES TERMINATED BY '\\n';\n",
    "\n",
    "LOAD DATA LOCAL INPATH 'data.tsv' INTO TABLE t0;\n",
    "\n",
    "-- SELECT * FROM t0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 1\n",
    "\n",
    "Escriba una consulta que calcule la cantidad de registros por clave de la columna 3. En otras palabras, cuántos registros hay que tengan la clave `aaa`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT key, COUNT(key)\n",
      "FROM \n",
      "(SELECT explode(c3) as (key, value) FROM t0) L\n",
      "GROUP BY key;\n",
      "Query ID = marvin_20181119081921_c027501b-a055-46af-95a4-45aa79854260\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2018-11-19 08:19:27,111 Stage-1 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local256407413_0001\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 0 msec\n",
      "OK\n",
      "aaa\t13\n",
      "bbb\t16\n",
      "ccc\t23\n",
      "ddd\t23\n",
      "eee\t15\n",
      "fff\t20\n",
      "ggg\t13\n",
      "hhh\t16\n",
      "iii\t18\n",
      "jjj\t18\n",
      "Time taken: 5.24 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT key, COUNT(key)\n",
    "FROM \n",
    "(SELECT explode(c3) as (key, value) FROM t0) L\n",
    "GROUP BY key;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 2\n",
    "\n",
    "Escriba una consulta que retorne la primera columna, la cantidad de elementos en la columna 2 y la cantidad de elementos en la columna 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT C1,\n",
      "SIZE(c2),\n",
      "SIZE(c3)\n",
      "\n",
      "FROM t0\n",
      ";\n",
      "OK\n",
      "E\t3\t5\n",
      "A\t3\t4\n",
      "B\t4\t4\n",
      "A\t2\t4\n",
      "C\t4\t4\n",
      "A\t2\t5\n",
      "A\t3\t6\n",
      "B\t2\t3\n",
      "E\t4\t6\n",
      "B\t4\t6\n",
      "C\t4\t5\n",
      "C\t4\t3\n",
      "D\t4\t5\n",
      "E\t2\t3\n",
      "B\t2\t5\n",
      "D\t2\t4\n",
      "E\t3\t6\n",
      "D\t2\t3\n",
      "E\t4\t3\n",
      "E\t2\t3\n",
      "E\t2\t3\n",
      "E\t3\t3\n",
      "D\t3\t3\n",
      "A\t3\t5\n",
      "E\t2\t6\n",
      "E\t3\t6\n",
      "A\t3\t3\n",
      "E\t3\t5\n",
      "A\t2\t5\n",
      "C\t4\t6\n",
      "A\t2\t5\n",
      "D\t2\t6\n",
      "E\t2\t4\n",
      "B\t3\t6\n",
      "B\t3\t5\n",
      "D\t2\t3\n",
      "B\t2\t5\n",
      "C\t4\t3\n",
      "E\t2\t3\n",
      "E\t3\t3\n",
      "Time taken: 0.164 seconds, Fetched: 40 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT C1,\n",
    "SIZE(c2),\n",
    "SIZE(c3)\n",
    "FROM t0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 3\n",
    "\n",
    "Escriba una consulta que compute la cantidad de registros por letra de la columna 2 y clave de la columna 3; esto es, por ejemplo, la cantidad de registros en tienen la letra `a` en la columna 2 y la clave `aaa` en la columna 3 es:\n",
    "\n",
    "     a    aaa    5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT llave2, llave1, count(*)\n",
      "FROM t0\n",
      "\n",
      "LATERAL VIEW EXPLODE(c3) dummy as llave1, valor\n",
      "LATERAL VIEW EXPLODE(c2) dummy as llave2\n",
      "\n",
      "GROUP BY llave1, llave2\n",
      "ORDER BY llave2 ASC, llave1 ASC;\n",
      "Query ID = marvin_20181119081941_69b5881d-b851-4bd0-aa45-cdf525cc5247\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2018-11-19 08:19:44,771 Stage-1 map = 100%,  reduce = 0%\n",
      "2018-11-19 08:19:46,785 Stage-1 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local2068376797_0002\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2018-11-19 08:19:48,725 Stage-2 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local2120209332_0003\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Stage-Stage-2:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 0 msec\n",
      "OK\n",
      "a\taaa\t5\n",
      "a\tbbb\t7\n",
      "a\tccc\t13\n",
      "a\tddd\t13\n",
      "a\teee\t7\n",
      "a\tfff\t10\n",
      "a\tggg\t8\n",
      "a\thhh\t8\n",
      "a\tiii\t7\n",
      "a\tjjj\t10\n",
      "b\taaa\t4\n",
      "b\tbbb\t7\n",
      "b\tccc\t5\n",
      "b\tddd\t7\n",
      "b\teee\t5\n",
      "b\tfff\t8\n",
      "b\tggg\t4\n",
      "b\thhh\t7\n",
      "b\tiii\t7\n",
      "b\tjjj\t5\n",
      "c\taaa\t5\n",
      "c\tbbb\t4\n",
      "c\tccc\t12\n",
      "c\tddd\t9\n",
      "c\teee\t6\n",
      "c\tfff\t8\n",
      "c\tggg\t7\n",
      "c\thhh\t7\n",
      "c\tiii\t8\n",
      "c\tjjj\t8\n",
      "d\taaa\t4\n",
      "d\tbbb\t6\n",
      "d\tccc\t7\n",
      "d\tddd\t5\n",
      "d\teee\t6\n",
      "d\tfff\t8\n",
      "d\tggg\t6\n",
      "d\thhh\t4\n",
      "d\tiii\t9\n",
      "d\tjjj\t8\n",
      "e\taaa\t3\n",
      "e\tbbb\t8\n",
      "e\tccc\t9\n",
      "e\tddd\t7\n",
      "e\teee\t7\n",
      "e\tfff\t9\n",
      "e\tggg\t4\n",
      "e\thhh\t4\n",
      "e\tiii\t8\n",
      "e\tjjj\t3\n",
      "f\taaa\t8\n",
      "f\tbbb\t10\n",
      "f\tccc\t13\n",
      "f\tddd\t17\n",
      "f\teee\t11\n",
      "f\tfff\t11\n",
      "f\tggg\t9\n",
      "f\thhh\t10\n",
      "f\tiii\t10\n",
      "f\tjjj\t12\n",
      "g\taaa\t3\n",
      "g\tbbb\t3\n",
      "g\tccc\t6\n",
      "g\tddd\t5\n",
      "g\teee\t4\n",
      "g\tfff\t5\n",
      "g\tggg\t4\n",
      "g\thhh\t3\n",
      "g\tiii\t4\n",
      "g\tjjj\t6\n",
      "Time taken: 6.872 seconds, Fetched: 70 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "SELECT l2, l1, count(*)\n",
    "FROM t0\n",
    "LATERAL VIEW EXPLODE(c3) dummy as l1, valor\n",
    "LATERAL VIEW EXPLODE(c2) dummy as l2\n",
    "GROUP BY l1, l2\n",
    "ORDER BY l2 ASC, l1 ASC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data.* pig_*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
